# share

1. `run_VPCL_pretrain.py` for running pretraining.

2. `run_scratch.py` for running training from scratch.

3. `run_finetune.py` for running finetuning after pretraining.


The finetune and training from scratch get similar performance because these pretraining datasets share low similarity.
